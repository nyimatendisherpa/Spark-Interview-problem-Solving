{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data pivoting in Pyspark\n",
        "problem: we have a pyspark DataFrame that stores monthly sales data of products across different regions.Our task is to pivot this data to show total monthly sales per product, where each month becomes column"
      ],
      "metadata": {
        "id": "8EFhvfJl5D2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum"
      ],
      "metadata": {
        "id": "z3FYXgqX5jRg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark=SparkSession.builder.appName(\"PivotExample\").getOrCreate()"
      ],
      "metadata": {
        "id": "t0hG6mj05isb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(\"abc1\", \"South\", \"Jan\", 1000),\n",
        " (\"abc1\", \"North\", \"Jan\", 1200),\n",
        " (\"abc1\", \"South\", \"Feb\", 1300),\n",
        " (\"abc2\", \"North\", \"Jan\", 900),\n",
        " (\"abc2\", \"South\", \"Feb\", 1100),\n",
        " (\"abc2\", \"North\", \"Feb\", 1000),\n",
        " (\"def1\", \"East\", \"jun\" ,1200),\n",
        "(\"abc1\", \"South\", \"Feb\", 1000),\n",
        "\n",
        "    ]"
      ],
      "metadata": {
        "id": "frCfeZf357S-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns=[\"product_id\",\"region\",\"month\",\"sales_amount\"]"
      ],
      "metadata": {
        "id": "yI1T2YAi552m"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.createDataFrame(data,columns)"
      ],
      "metadata": {
        "id": "uobuekDx6-99"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group and pivot"
      ],
      "metadata": {
        "id": "bl7k83JG7IRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df=df.groupBy(\"product_id\").pivot(\"month\").agg(sum(\"sales_amount\")).fillna(0)\n",
        "pivot_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga2cIg0d7Gzy",
        "outputId": "3eff52d2-62b0-469d-c993-f8587015fcb7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[product_id: string, Feb: bigint, Jan: bigint, jun: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyjTi-yc8kGC",
        "outputId": "e6446572-d486-493a-e01b-d92f979fdd2a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+----+----+\n",
            "|product_id| Feb| Jan| jun|\n",
            "+----------+----+----+----+\n",
            "|      abc1|2300|2200|   0|\n",
            "|      def1|   0|   0|1200|\n",
            "|      abc2|2100| 900|   0|\n",
            "+----------+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FMgMj2qu8-Gy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}